{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2efaa9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aba53beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dolfinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe57b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==0.22.1\n",
      "  Using cached scikit_learn-0.22.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /home/joshua/Desktop/projectvenv/researchvenv/lib/python3.7/site-packages (from scikit-learn==0.22.1) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/joshua/Desktop/projectvenv/researchvenv/lib/python3.7/site-packages (from scikit-learn==0.22.1) (1.3.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /home/joshua/Desktop/projectvenv/researchvenv/lib/python3.7/site-packages (from scikit-learn==0.22.1) (1.4.1)\n",
      "\u001b[31mERROR: imbalanced-learn 0.12.2 has requirement scikit-learn>=1.0.2, but you'll have scikit-learn 0.22.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: imbalanced-learn 0.12.2 has requirement scipy>=1.5.0, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "Successfully installed scikit-learn-0.22.1\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/home/joshua/Desktop/projectvenv/researchvenv/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn==0.22.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577c42b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5419a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x0 = np.array([1, 0])  # start point of integration\n",
    "t0 = 0\n",
    "t1 = 200\n",
    "d = 2  # dimension of the ODE state space\n",
    "\n",
    "#step_sizes = [0.025, 0.029, 0.033, 0.039, 0.045, 0.052, 0.060, 0.070]  # the NN will choose from these step sizes\n",
    "step_sizes= np.linspace(0.02, 0.15, 20)\n",
    "tol = 0.0001  # target integration error tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9af83388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptive.integrator import RKDP\n",
    "integrator = RKDP()\n",
    "\n",
    "nodes_per_step = 6  # the RKDP method has 6 stages\n",
    "dim_action = len(step_sizes)  # the dimension of the action space of the NN\n",
    "memory = 0  # how many integration steps the predictor can look back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19b82178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.mean_ = np.zeros((nodes_per_step * d + 1) * (memory + 1))\n",
    "scaler.mean_[0] = -0.045\n",
    "scaler.scale_ = 10 * np.ones((nodes_per_step * d + 1) * (memory + 1))\n",
    "scaler.scale_[0] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fba3cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.mean_ = np.zeros((nodes_per_step * d + 1) * (memory + 1))\n",
    "scaler.mean_[0] = -0.045\n",
    "scaler.scale_ = 10 * np.ones((nodes_per_step * d + 1) * (memory + 1))\n",
    "scaler.scale_[0] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56310ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive rewards (error is smaller than tol=0.0001):\n",
      "error, step size, reward\n",
      "1e-05 0.02 0.1\n",
      "1e-05 0.03368421052631579 0.592\n",
      "1e-05 0.15 2.0\n",
      "\n",
      "negative rewards (error is larger than tol=0.0001):\n",
      "error, step size, reward\n",
      "0.0002 0.02 -0.301\n",
      "0.001 0.02 -1.0\n",
      "0.01 0.02 -2.0\n"
     ]
    }
   ],
   "source": [
    "from adaptive.reward_functions import RewardLog10\n",
    "\n",
    "step_size_range = (step_sizes[0], step_sizes[-1])  # range of expected step sizes\n",
    "reward_range = (0.1, 2)  # range of positive rewards\n",
    "\n",
    "reward_fun = RewardLog10(error_tol=tol, step_size_range=step_size_range, reward_range=reward_range)\n",
    "\n",
    "# print some example rewards\n",
    "errors = [0.1 * tol, 0.1 * tol, 0.1 * tol]\n",
    "hs = [step_sizes[0], step_sizes[2], step_sizes[-1]]\n",
    "print(f\"positive rewards (error is smaller than tol={tol}):\")\n",
    "print(\"error, step size, reward\")\n",
    "for er, h in zip(errors, hs):\n",
    "    print(er, h, round(reward_fun(er, h), 3))\n",
    "print()\n",
    "errors = [2 * tol, 10 * tol, 100 * tol]\n",
    "hs = [step_sizes[0], step_sizes[0], step_sizes[0]]\n",
    "print(f\"negative rewards (error is larger than tol={tol}):\")\n",
    "print(\"error, step size, reward\")\n",
    "for er, h in zip(errors, hs):\n",
    "    print(er, h, round(reward_fun(er, h), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60b1b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptive.environments import ODEEnv\n",
    "from functions import LorenzSystem, VanDerPol\n",
    "\n",
    "env = ODEEnv(fun=VanDerPol(), max_iterations=10000, initial_step_size=step_sizes[0],\n",
    "             step_size_range=step_size_range, reward_fun=reward_fun,\n",
    "             error_tol=tol, nodes_per_integ=nodes_per_step, memory=memory, x0=x0, t0=t0, max_dist=t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dde0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adaptive.experience import ExperienceODE\n",
    "\n",
    "experience = ExperienceODE(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b89ed85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(actions, eps, dim_action):\n",
    "    \"\"\"\n",
    "    With probability 0.5*eps choose the action one above the favored action and with probability 0.5*eps the action\n",
    "    below the favored.\n",
    "    Otherwise choose the favored action.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actions : np.ndarray\n",
    "    eps : float\n",
    "    dim_action : int\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "    \"\"\"\n",
    "    favored = np.argmax(actions)\n",
    "    rn = np.random.sample()\n",
    "\n",
    "    if rn < 0.5 * eps:\n",
    "        return min(favored + 1, dim_action - 1)\n",
    "    if rn < eps:\n",
    "        return max(favored - 1, 0)\n",
    "\n",
    "    return favored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d6ac24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,13) (3,) (1,13) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(episode))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# get action from actor\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     actions \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mget_actions(state)  \u001b[38;5;66;03m# Q-value of each step size\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     action \u001b[38;5;241m=\u001b[39m choose_action(actions, eps, dim_action)  \u001b[38;5;66;03m# randomize step size\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39maction_to_stepsize(action)\n",
      "File \u001b[0;32m~/Desktop/projectvenv/upload/quadrature-ML/adaptive/predictor.py:254\u001b[0m, in \u001b[0;36mPredictorQODE.get_actions\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03mReturn the value of each possible action.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03mnp.ndarray\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    253\u001b[0m states \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([state\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idx) \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m states])\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform([states]))\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/envs/advfea/lib/python3.11/site-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/advfea/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n\u001b[0;32m-> 1062\u001b[0m         X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[1;32m   1064\u001b[0m         X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,13) (3,) (1,13) "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "num_episodes = 3\n",
    "gamma = 0  # discount factor for future rewards\n",
    "\n",
    "for episode in range(3):\n",
    "    state = env.reset(integrator=integrator)\n",
    "    done = False\n",
    "    eps = 0.5\n",
    "    print('episode: {}'.format(episode))\n",
    "\n",
    "    while not done:\n",
    "        # get action from actor\n",
    "        actions = predictor.get_actions(state)  # Q-value of each step size\n",
    "        action = choose_action(actions, eps, dim_action)  # randomize step size\n",
    "        step_size = predictor.action_to_stepsize(action)\n",
    "\n",
    "        # execute action\n",
    "        next_state, reward, done, _ = env.iterate(step_size, integrator)\n",
    "\n",
    "        # find target values (more correct Q-values) for NN output\n",
    "        action_next_state = predictor.get_actions(next_state)\n",
    "        target = reward + gamma * np.max(action_next_state)\n",
    "        target_actions = actions.squeeze()\n",
    "        target_actions[action] = target\n",
    "        # append target values to experience (batch)\n",
    "        experience.append(state=state, target=target_actions)\n",
    "        \n",
    "        # train the NN\n",
    "        if experience.is_full() or done:\n",
    "            states, targets = experience.get_samples()\n",
    "            predictor.train_on_batch(states, targets)\n",
    "            experience.reset()\n",
    "\n",
    "        state = next_state.copy()\n",
    "    \n",
    "    # uncomment if you want to train the NN yourself and save the weights:\n",
    "    # predictor.model.save_weights('predictorODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e504807",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File format not supported: filepath=predictorODE. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the already trained NN\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictor\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m build_value_modelODE(dim_state\u001b[38;5;241m=\u001b[39mnodes_per_step \u001b[38;5;241m*\u001b[39m d \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      3\u001b[0m                                        dim_action\u001b[38;5;241m=\u001b[39mdim_action,\n\u001b[1;32m      4\u001b[0m                                        memory\u001b[38;5;241m=\u001b[39mmemory,\n\u001b[1;32m      5\u001b[0m                                        filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictorODE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projectvenv/upload/quadrature-ML/adaptive/build_models.py:79\u001b[0m, in \u001b[0;36mbuild_value_modelODE\u001b[0;34m(dim_state, dim_action, filename, lr, memory)\u001b[0m\n\u001b[1;32m     76\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# !huber loss!\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(filename)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/advfea/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/advfea/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_weights\u001b[0;34m(model, filepath, skip_mismatch, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m             legacy_h5_format\u001b[38;5;241m.\u001b[39mload_weights_from_hdf5_group(f, model)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` and `.weights.h5` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles, or legacy V1/V2 `.h5` files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: File format not supported: filepath=predictorODE. Keras 3 only supports V3 `.keras` and `.weights.h5` files, or legacy V1/V2 `.h5` files."
     ]
    }
   ],
   "source": [
    "# load the already trained NN\n",
    "predictor.model = build_value_modelODE(dim_state=nodes_per_step * d + 1,\n",
    "                                       dim_action=dim_action,\n",
    "                                       memory=memory,\n",
    "                                       filename='predictorODE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advfea",
   "language": "python",
   "name": "advfea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
